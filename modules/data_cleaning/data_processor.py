#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æ•°æ®æ¸…æ´—å¤„ç†å·¥å…·

åŠŸèƒ½ï¼š
- è¯»å–å¾…å¤„ç†å®ä½“æ•°æ®
- ä¸å‚è€ƒæ•°æ®æ¸…å•è¿›è¡Œæ¯”å¯¹
- æ·»åŠ é¢å¤–å±æ€§ä¿¡æ¯
- è¾“å‡ºå¤„ç†åçš„æ•°æ®æ–‡ä»¶
"""

import pandas as pd
import os
import re
import time
import csv
from pathlib import Path
import chardet

# é…ç½®å¸¸é‡
DEFAULT_ENCODINGS = ['utf-8-sig', 'utf-8', 'gbk', 'gb2312', 'cp1252', 'ansi', 'cp936', 'latin1']
OUTPUT_ENCODING = 'utf-8-sig'
SUPPORTED_EXTENSIONS = ['.csv', '.xlsx', '.xls']

def detect_file_encoding(file_path):
    """
    è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç 
    
    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„
    
    Returns:
        str: æ£€æµ‹åˆ°çš„ç¼–ç 
    """
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)  # è¯»å–å‰10KBç”¨äºæ£€æµ‹
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            confidence = result['confidence']
            
            # å¤„ç†ANSIç¼–ç çš„ç‰¹æ®Šæƒ…å†µ
            if encoding and encoding.lower() in ['ascii', 'windows-1252']:
                encoding = 'cp1252'  # Windows ANSI
            elif encoding and 'gb' in encoding.lower():
                encoding = 'gbk'  # ä¸­æ–‡ç¼–ç 
            
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç : {encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
            return encoding
    except Exception as e:
        print(f"âš ï¸  ç¼–ç æ£€æµ‹å¤±è´¥: {e}")
        return None

def read_file_with_encoding(file_path, encodings=None):
    """
    å°è¯•ç”¨å¤šç§ç¼–ç è¯»å–CSV/Excelæ–‡ä»¶ï¼Œæ”¯æŒå¤„ç†åŒ…å«æ¢è¡Œç¬¦çš„å­—æ®µ
    
    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„
        encodings (list): ç¼–ç åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨DEFAULT_ENCODINGS
    
    Returns:
        tuple: (DataFrame, ä½¿ç”¨çš„ç¼–ç /æ–‡ä»¶ç±»å‹)
    
    Raises:
        ValueError: æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    file_path = Path(file_path)
    file_extension = file_path.suffix.lower()
    
    # å¤„ç†Excelæ–‡ä»¶
    if file_extension in ['.xlsx', '.xls']:
        try:
            print(f"ğŸ“– æ­£åœ¨è¯»å–Excelæ–‡ä»¶: {file_path.name}")
            if file_extension == '.xlsx':
                df = pd.read_excel(file_path, engine='openpyxl')
            else:
                df = pd.read_excel(file_path, engine='xlrd')
            
            print(f"âœ… Excelæ–‡ä»¶è¯»å–æˆåŠŸ: {file_path.name}")
            return df, f"Excel-{file_extension}"
        except Exception as e:
            print(f"âŒ Excelæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            raise ValueError(f"æ— æ³•è¯»å–Excelæ–‡ä»¶ {file_path}: {e}")
    
    # å¤„ç†CSVæ–‡ä»¶
    if encodings is None:
        encodings = DEFAULT_ENCODINGS.copy()
    
    # é¦–å…ˆå°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
    detected_encoding = detect_file_encoding(file_path)
    if detected_encoding and detected_encoding not in encodings:
        encodings.insert(0, detected_encoding)
    
    for encoding in encodings:
        try:
            # ç‰¹æ®Šå¤„ç†ANSIç¼–ç 
            if encoding.lower() == 'ansi':
                encoding = 'cp1252'  # Windows ANSIé€šå¸¸æ˜¯cp1252
            
            # ä½¿ç”¨quoting=csv.QUOTE_ALLæ¥æ­£ç¡®å¤„ç†åŒ…å«æ¢è¡Œç¬¦çš„å­—æ®µ
            df = pd.read_csv(
                file_path, 
                encoding=encoding,
                quoting=1,  # csv.QUOTE_ALL - å¤„ç†æ‰€æœ‰å¼•å·
                skipinitialspace=True,  # è·³è¿‡åˆ†éš”ç¬¦åçš„ç©ºæ ¼
                keep_default_na=False,  # ä¿æŒç©ºå€¼ä¸ºç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯NaN
                na_filter=False,  # ä¸è‡ªåŠ¨è½¬æ¢NAå€¼
                on_bad_lines='skip'  # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œ
            )
            print(f"ğŸ“– æ–‡ä»¶ {file_path.name} ä½¿ç”¨ç¼–ç : {encoding}")
            return df, encoding
        except (UnicodeDecodeError, UnicodeError, FileNotFoundError, pd.errors.ParserError) as e:
            print(f"âš ï¸  ç¼–ç  {encoding} è¯»å–å¤±è´¥: {e}")
            continue
    
    raise ValueError(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œå·²å°è¯•ç¼–ç : {encodings}")

# ä¿æŒå‘åå…¼å®¹æ€§çš„åˆ«å
read_csv_with_encoding = read_file_with_encoding

def generate_output_filename(input_filename):
    """
    æ ¹æ®è¾“å…¥æ–‡ä»¶åç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    
    Args:
        input_filename (str): è¾“å…¥æ–‡ä»¶å
    
    Returns:
        str: ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶å
    """
    # è·å–ä¸å¸¦æ‰©å±•åçš„æ–‡ä»¶å
    name_without_ext = Path(input_filename).stem
    
    # è·å–å½“å‰æ—¶é—´æˆ³
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # ç”Ÿæˆæ–°çš„æ–‡ä»¶åï¼šåŸæ–‡ä»¶å_å·²å¤„ç†_æ—¶é—´æˆ³.csv
    output_filename = f"{name_without_ext}_å·²å¤„ç†_{timestamp}.csv"
    
    return output_filename

def find_data_files(directory):
    """
    æŸ¥æ‰¾ç›®å½•ä¸­çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼ˆCSVå’ŒExcelï¼‰
    
    Args:
        directory (Path): ç›®å½•è·¯å¾„
    
    Returns:
        list: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    data_files = []
    if directory.exists():
        for ext in SUPPORTED_EXTENSIONS:
            data_files.extend(list(directory.glob(f"*{ext}")))
            data_files.extend(list(directory.glob(f"*{ext.upper()}")))  # æ”¯æŒå¤§å†™æ‰©å±•å
    return sorted(data_files)  # æ’åºç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´

# ä¿æŒå‘åå…¼å®¹æ€§çš„åˆ«å
find_csv_files = find_data_files

def merge_empty_rows(df, chinese_name_col, cas_col=None):
    """
    åˆå¹¶ç©ºè¡Œï¼šå¦‚æœæŸè¡Œçš„ä¸­æ–‡åå’ŒCASå·éƒ½ä¸ºç©ºï¼Œå°†å…¶ä¸ä¸Šä¸€è¡Œåˆå¹¶
    
    Args:
        df (pd.DataFrame): è¾“å…¥æ•°æ®æ¡†
        chinese_name_col (str): ä¸­æ–‡ååˆ—å
        cas_col (str): CASå·åˆ—åï¼ˆå¯é€‰ï¼‰
    
    Returns:
        pd.DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
    """
    if len(df) <= 1:
        return df
    
    print("ğŸ”§ æ­£åœ¨æ£€æŸ¥å¹¶åˆå¹¶ç©ºè¡Œ...")
    
    # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    df_merged = df.copy()
    
    # ç¡®ä¿ç›¸å…³åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
    df_merged[chinese_name_col] = df_merged[chinese_name_col].astype(str).fillna('').str.strip()
    if cas_col:
        df_merged[cas_col] = df_merged[cas_col].astype(str).fillna('').str.strip()
    
    # æ ‡è®°éœ€è¦åˆ é™¤çš„è¡Œ
    rows_to_delete = []
    merge_count = 0
    
    for i in range(1, len(df_merged)):
        current_row = df_merged.iloc[i]
        prev_row = df_merged.iloc[i-1]
        
        # æ£€æŸ¥å½“å‰è¡Œçš„ä¸­æ–‡åæ˜¯å¦ä¸ºç©º
        chinese_name_empty = (
            current_row[chinese_name_col] == '' or 
            current_row[chinese_name_col] == 'nan' or 
            pd.isna(current_row[chinese_name_col])
        )
        
        # æ£€æŸ¥å½“å‰è¡Œçš„CASå·æ˜¯å¦ä¸ºç©ºï¼ˆå¦‚æœæœ‰CASåˆ—ï¼‰
        cas_empty = True
        if cas_col:
            cas_empty = (
                current_row[cas_col] == '' or 
                current_row[cas_col] == 'nan' or 
                pd.isna(current_row[cas_col])
            )
        
        # å¦‚æœä¸­æ–‡åå’ŒCASå·éƒ½ä¸ºç©ºï¼Œåˆ™åˆå¹¶åˆ°ä¸Šä¸€è¡Œ
        if chinese_name_empty and cas_empty:
            merge_count += 1
            
            # åˆå¹¶æ‰€æœ‰éç©ºå­—æ®µåˆ°ä¸Šä¸€è¡Œ
            for col in df_merged.columns:
                current_value = str(current_row[col]).strip()
                prev_value = str(prev_row[col]).strip()
                
                # å¦‚æœå½“å‰è¡Œçš„å€¼ä¸ä¸ºç©ºä¸”ä¸ä¸Šä¸€è¡Œä¸åŒï¼Œåˆ™è¿½åŠ 
                if (current_value != '' and 
                    current_value not in ['nan', 'None'] and 
                    not pd.isna(current_value)):
                    
                    if (prev_value == '' or 
                        prev_value in ['nan', 'None'] or 
                        pd.isna(prev_value)):
                        # ä¸Šä¸€è¡Œä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨å½“å‰è¡Œçš„å€¼
                        df_merged.iloc[i-1, df_merged.columns.get_loc(col)] = current_value
                    elif current_value != prev_value:
                        # ä¸¤è¡Œéƒ½æœ‰å€¼ä¸”ä¸åŒï¼Œç”¨ç©ºæ ¼è¿æ¥
                        merged_value = f"{prev_value} {current_value}"
                        df_merged.iloc[i-1, df_merged.columns.get_loc(col)] = merged_value
            
            # æ ‡è®°å½“å‰è¡Œä¸ºåˆ é™¤
            rows_to_delete.append(i)
    
    # åˆ é™¤å·²åˆå¹¶çš„ç©ºè¡Œ
    if rows_to_delete:
        df_merged = df_merged.drop(df_merged.index[rows_to_delete]).reset_index(drop=True)
        print(f"âœ… å·²åˆå¹¶ {merge_count} ä¸ªç©ºè¡Œï¼Œåˆ é™¤äº† {len(rows_to_delete)} è¡Œ")
        print(f"ğŸ“Š æ•°æ®è¡Œæ•°ä» {len(df)} å‡å°‘åˆ° {len(df_merged)}")
    else:
        print("ğŸ“Š æœªå‘ç°éœ€è¦åˆå¹¶çš„ç©ºè¡Œ")
    
    return df_merged

def merge_empty_columns(df, data_name):
    """
    æ£€æŸ¥å¹¶åˆå¹¶ç©ºåˆ—åçš„åˆ—ï¼Œæ ¹æ®è¯­è¨€ç±»å‹ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰æ™ºèƒ½åˆå¹¶åˆ°æœ€è¿‘çš„ç›¸åŒè¯­è¨€ç±»å‹åˆ—ï¼Œç„¶ååˆ é™¤æ‰€æœ‰å‰©ä½™çš„ç©ºåˆ—
    
    åˆå¹¶è§„åˆ™ï¼š
    1. åªå°†ç©ºåˆ—åˆå¹¶åˆ°å·²æœ‰å®šä¹‰çš„åˆ—ï¼ˆéç©ºåˆ—åï¼‰
    2. æ ¹æ®åˆ—å†…å®¹çš„è¯­è¨€ç±»å‹ï¼ˆä¸­æ–‡/è‹±æ–‡/æ··åˆï¼‰è¿›è¡ŒåŒ¹é…
    3. ä¼˜å…ˆé€‰æ‹©è·ç¦»æœ€è¿‘çš„ç›¸åŒè¯­è¨€ç±»å‹åˆ—
    4. åªåˆå¹¶éé‡å¤çš„å†…å®¹ï¼Œé¿å…è¦†ç›–å·²æœ‰æ•°æ®
    
    Args:
        df (pd.DataFrame): æ•°æ®æ¡†
        data_name (str): æ•°æ®åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º
    
    Returns:
        pd.DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
    """
    original_cols = df.columns.tolist()
    empty_indices = []
    
    # æ‰¾åˆ°æ‰€æœ‰ç©ºåˆ—åçš„ç´¢å¼•
    for i, col in enumerate(original_cols):
        col_str = str(col).strip().lower()
        if (pd.isna(col) or 
            col_str == '' or 
            col_str == 'nan' or 
            col_str.startswith('unnamed:') or
            col_str.startswith('unnamed ') or
            col_str == 'unnamed'):
            empty_indices.append(i)
    
    if not empty_indices:
        print(f"   âœ… {data_name} æœªå‘ç°ç©ºåˆ—å")
        return df
    
    print(f"   ğŸ” {data_name} å‘ç° {len(empty_indices)} ä¸ªç©ºåˆ—åï¼Œä½ç½®: {empty_indices}")
    
    # å¼ºåˆ¶åˆ é™¤æ‰€æœ‰ç©ºåˆ—ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    cols_to_drop = []
    
    # æŒ‰ç´¢å¼•é™åºåˆ é™¤ï¼Œé¿å…ç´¢å¼•å˜åŒ–å½±å“
    empty_indices.sort(reverse=True)
    for idx in empty_indices:
        col_name = original_cols[idx]
        df = df.drop(df.columns[idx], axis=1)
        cols_to_drop.append(f"åˆ—{idx}({col_name})")
    
    print(f"   ğŸ—‘ï¸  {data_name} å¼ºåˆ¶åˆ é™¤æ‰€æœ‰ç©ºåˆ—: {len(empty_indices)} ä¸ª")
    for col_info in cols_to_drop:
        print(f"      - åˆ é™¤: {col_info}")
    print(f"   ğŸ“Š æœ€ç»ˆåˆ—æ•°: {len(df.columns)} (åŸ: {len(original_cols)})")
    
    return df

def merge_alias_columns(df, data_name):
    """
    åˆå¹¶æ•°æ®æ¡†ä¸­çš„åˆ«ååˆ—ï¼Œä½†ä¿ç•™è‹±æ–‡åˆ«åä½œä¸ºç‹¬ç«‹åˆ—
    
    è§„åˆ™ï¼š
    - å°†ç³»ç»Ÿç”Ÿæˆçš„"åˆ«å"åˆ—åˆå¹¶åˆ°"ä¸­æ–‡åˆ«å"åˆ—ä¸­
    - ä¿ç•™"è‹±æ–‡åˆ«å"åˆ—ä½œä¸ºç‹¬ç«‹åˆ—
    - å¦‚æœæ²¡æœ‰"ä¸­æ–‡åˆ«å"åˆ—ï¼Œå°†"åˆ«å"åˆ—é‡å‘½åä¸º"ä¸­æ–‡åˆ«å"
    
    Args:
        df (pd.DataFrame): æ•°æ®æ¡†
        data_name (str): æ•°æ®åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º
    
    Returns:
        pd.DataFrame: å¤„ç†åçš„æ•°æ®æ¡†
    """
    # æŸ¥æ‰¾ç›¸å…³çš„åˆ«ååˆ—
    chinese_alias_col = None
    english_alias_col = None
    alias_col = None
    
    for col in df.columns:
        col_str = str(col).strip()
        if col_str == 'ä¸­æ–‡åˆ«å':
            chinese_alias_col = col
        elif col_str == 'è‹±æ–‡åˆ«å':
            english_alias_col = col
        elif col_str == 'åˆ«å':
            alias_col = col
    
    alias_cols_found = []
    if chinese_alias_col:
        alias_cols_found.append(f"ä¸­æ–‡åˆ«å: {chinese_alias_col}")
    if english_alias_col:
        alias_cols_found.append(f"è‹±æ–‡åˆ«å: {english_alias_col}")
    if alias_col:
        alias_cols_found.append(f"åˆ«å: {alias_col}")
    
    if not alias_cols_found:
        print(f"   â„¹ï¸  {data_name} æœªå‘ç°ä»»ä½•åˆ«ååˆ—")
        return df
    
    print(f"   ï¿½ {data_name} å‘ç°åˆ«ååˆ—: {', '.join(alias_cols_found)}")
    
    # å¤„ç†åˆ«ååˆ—åˆå¹¶
    merge_operations = []
    
    # æƒ…å†µ1ï¼šå¦‚æœåŒæ—¶å­˜åœ¨"ä¸­æ–‡åˆ«å"å’Œ"åˆ«å"åˆ—ï¼Œå°†"åˆ«å"åˆå¹¶åˆ°"ä¸­æ–‡åˆ«å"
    if chinese_alias_col and alias_col:
        print(f"   ğŸ“‹ {data_name} å°†'åˆ«å'åˆ—åˆå¹¶åˆ°'ä¸­æ–‡åˆ«å'åˆ—")
        
        # è·å–ä¸¤åˆ—çš„å†…å®¹
        chinese_content = df[chinese_alias_col].astype(str).fillna('').str.strip()
        alias_content = df[alias_col].astype(str).fillna('').str.strip()
        
        # åˆå¹¶å†…å®¹
        merged_content = []
        merge_count = 0
        
        for i in range(len(df)):
            chinese_val = chinese_content.iloc[i]
            alias_val = alias_content.iloc[i]
            
            # æ¸…ç†ç©ºå€¼
            if chinese_val.lower() in ['', 'nan', 'null']:
                chinese_val = ''
            if alias_val.lower() in ['', 'nan', 'null']:
                alias_val = ''
            
            # åˆå¹¶é€»è¾‘
            if chinese_val == '' and alias_val == '':
                merged_content.append('')
            elif chinese_val == '':
                merged_content.append(alias_val)
                if alias_val != '':
                    merge_count += 1
            elif alias_val == '':
                merged_content.append(chinese_val)
            else:
                # æ£€æŸ¥æ˜¯å¦é‡å¤
                if alias_val not in chinese_val.split('ï¼›') and alias_val not in chinese_val.split(';'):
                    merged_content.append(chinese_val + 'ï¼›' + alias_val)
                    merge_count += 1
                else:
                    merged_content.append(chinese_val)
        
        # æ›´æ–°ä¸­æ–‡åˆ«ååˆ—
        df[chinese_alias_col] = merged_content
        
        # åˆ é™¤åˆ«ååˆ—
        df = df.drop(columns=[alias_col])
        merge_operations.append(f"åˆ—'åˆ«å' -> åˆ—'ä¸­æ–‡åˆ«å' [åˆå¹¶{merge_count}ä¸ªå€¼]")
        
    # æƒ…å†µ2ï¼šå¦‚æœåªæœ‰"åˆ«å"åˆ—è€Œæ²¡æœ‰"ä¸­æ–‡åˆ«å"åˆ—ï¼Œå°†"åˆ«å"åˆ—é‡å‘½åä¸º"ä¸­æ–‡åˆ«å"
    elif alias_col and not chinese_alias_col:
        print(f"   ğŸ“‹ {data_name} å°†'åˆ«å'åˆ—é‡å‘½åä¸º'ä¸­æ–‡åˆ«å'")
        df = df.rename(columns={alias_col: 'ä¸­æ–‡åˆ«å'})
        merge_operations.append("åˆ—'åˆ«å' -> é‡å‘½åä¸º'ä¸­æ–‡åˆ«å'")
    
    # è¾“å‡ºæ“ä½œç»“æœ
    if merge_operations:
        print(f"   âœ… {data_name} åˆ«ååˆ—å¤„ç†å®Œæˆ:")
        for op in merge_operations:
            print(f"      - {op}")
        
        # æ˜¾ç¤ºæœ€ç»ˆçš„åˆ«ååˆ—ç»“æ„
        final_alias_cols = []
        if 'ä¸­æ–‡åˆ«å' in df.columns:
            final_alias_cols.append('ä¸­æ–‡åˆ«å')
        if english_alias_col and english_alias_col in df.columns:
            final_alias_cols.append('è‹±æ–‡åˆ«å')
        
        if final_alias_cols:
            print(f"   ğŸ“Š ä¿ç•™åˆ«ååˆ—: {', '.join(final_alias_cols)}")
    else:
        print(f"   âœ… {data_name} åˆ«ååˆ—æ— éœ€å¤„ç†")
    
    return df

def process_data(input_file, hazardous_chemicals_file, output_file):
    """
    å¤„ç†åŒ–å­¦å“æ•°æ®ï¼Œä¸å›½å®¶å±åŒ–å“æ¸…å•è¿›è¡Œæ¯”å¯¹ï¼Œå¹¶æ·»åŠ æ–°åˆ—ã€‚

    Args:
        input_file (str): å¾…å¤„ç†æ•°æ®æ–‡ä»¶è·¯å¾„
        hazardous_chemicals_file (str): å›½å®¶å±åŒ–å“æ¸…å•æ–‡ä»¶è·¯å¾„
        output_file (str): å¤„ç†åæ–‡ä»¶çš„ä¿å­˜è·¯å¾„
    
    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    start_time = time.time()
    print("ğŸš€ å¼€å§‹å¤„ç†åŒ–å­¦å“æ•°æ®...")
    
    try:
        # è¯»å–æ–‡ä»¶
        print("ğŸ“‚ æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")
        df_input, input_encoding = read_file_with_encoding(input_file)
        df_hazardous, hazardous_encoding = read_file_with_encoding(hazardous_chemicals_file)

        # å¤„ç†ç©ºåˆ—åå’Œåˆå¹¶ç›¸åŒå†…å®¹çš„åˆ—
        print("ğŸ”§ æ­£åœ¨æ£€æŸ¥å¹¶å¤„ç†ç©ºåˆ—å...")
        df_input = merge_empty_columns(df_input, "å¾…å¤„ç†æ•°æ®")
        df_hazardous = merge_empty_columns(df_hazardous, "å±åŒ–å“æ¸…å•")

        # åˆå¹¶åˆ«åå’Œä¸­æ–‡åˆ«ååˆ—
        print("ğŸ”§ æ­£åœ¨åˆå¹¶åˆ«åå’Œä¸­æ–‡åˆ«ååˆ—...")
        df_input = merge_alias_columns(df_input, "å¾…å¤„ç†æ•°æ®")
        df_hazardous = merge_alias_columns(df_hazardous, "å±åŒ–å“æ¸…å•")

        # æ™ºèƒ½æ£€æµ‹åˆ—å
        print("ğŸ” æ­£åœ¨æ™ºèƒ½æ£€æµ‹åˆ—å...")
        
        # æ£€æµ‹å±åŒ–å“æ¸…å•çš„å“ååˆ—
        product_name_col = detect_column_names(df_hazardous, 'product_name')
        if not product_name_col:
            raise ValueError(f"å±åŒ–å“æ¸…å•æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å“åç›¸å…³åˆ—ã€‚å¯ç”¨åˆ—: {list(df_hazardous.columns)}")
        print(f"ğŸ“‹ å±åŒ–å“æ¸…å•å“ååˆ—: {product_name_col}")
        
        # æ£€æµ‹å±åŒ–å“æ¸…å•çš„åˆ«ååˆ—
        hazard_alias_col = detect_column_names(df_hazardous, 'alias')
        if not hazard_alias_col:
            print("âš ï¸  å±åŒ–å“æ¸…å•ä¸­æœªæ‰¾åˆ°åˆ«ååˆ—ï¼Œå°†ä½¿ç”¨å“åä½œä¸ºåˆ«å")
            hazard_alias_col = product_name_col
        else:
            print(f"ğŸ“‹ å±åŒ–å“æ¸…å•åˆ«ååˆ—: {hazard_alias_col}")
        
        # æ£€æµ‹å¾…å¤„ç†æ•°æ®çš„ä¸­æ–‡ååˆ—
        chinese_name_col = detect_column_names(df_input, 'chinese_name')
        if not chinese_name_col:
            raise ValueError(f"å¾…å¤„ç†æ•°æ®ä¸­æœªæ‰¾åˆ°ä¸­æ–‡åç›¸å…³åˆ—ã€‚å¯ç”¨åˆ—: {list(df_input.columns)}")
        print(f"ğŸ“‹ å¾…å¤„ç†æ•°æ®ä¸­æ–‡ååˆ—: {chinese_name_col}")
        
        # æ£€æµ‹å¾…å¤„ç†æ•°æ®çš„è‹±æ–‡ååˆ—ï¼ˆå¯é€‰ï¼‰
        english_name_col = detect_column_names(df_input, 'english_name')
        if english_name_col:
            print(f"ğŸ“‹ å¾…å¤„ç†æ•°æ®è‹±æ–‡ååˆ—: {english_name_col}")
        else:
            print("ğŸ“‹ æœªæ£€æµ‹åˆ°è‹±æ–‡ååˆ—ï¼Œå°†åœ¨ä¸­æ–‡ååç›´æ¥æ’å…¥åˆ«ååˆ—")

        # æ£€æµ‹CASå·åˆ—ï¼ˆç”¨äºè¾…åŠ©åŒ¹é…ï¼‰
        input_cas_col = detect_column_names(df_input, 'cas')
        hazard_cas_col = detect_column_names(df_hazardous, 'cas')
        
        if input_cas_col and hazard_cas_col:
            print(f"ğŸ“‹ å¾…å¤„ç†æ•°æ®CASå·åˆ—: {input_cas_col}")
            print(f"ğŸ“‹ å±åŒ–å“æ¸…å•CASå·åˆ—: {hazard_cas_col}")
            print("ğŸ” å°†ä½¿ç”¨ä¸­æ–‡å + CASå·è¿›è¡ŒåŒé‡åŒ¹é…")
        else:
            print("âš ï¸  CASå·åˆ—æ£€æµ‹ä¸å®Œæ•´ï¼Œä»…ä½¿ç”¨ä¸­æ–‡åè¿›è¡ŒåŒ¹é…")

        # æ˜¾ç¤ºæ•°æ®æ¡†è¯¦ç»†ä¿¡æ¯
        display_dataframe_info(df_input, "å¾…å¤„ç†æ•°æ®", {
            "ä¸­æ–‡ååˆ—": chinese_name_col,
            "è‹±æ–‡ååˆ—": english_name_col,
            "CASå·åˆ—": input_cas_col
        })
        
        display_dataframe_info(df_hazardous, "å±åŒ–å“æ¸…å•", {
            "å“ååˆ—": product_name_col,
            "åˆ«ååˆ—": hazard_alias_col,
            "CASå·åˆ—": hazard_cas_col
        })

        # ä¸ºäº†æ–¹ä¾¿æ¯”å¯¹ï¼Œæˆ‘ä»¬å…ˆä»å±åŒ–å“æ¸…å•çš„"å“å"ä¸­æå–å‡ºåŒ–å­¦å“åç§°å’Œæµ“åº¦
        # ä¾‹å¦‚ï¼šä» "å¯¹ä“åŸºåŒ–è¿‡æ°§æ°¢[72%ï¼œå«é‡â‰¤100%]" æå– "å¯¹ä“åŸºåŒ–è¿‡æ°§æ°¢" å’Œ "[72%ï¼œå«é‡â‰¤100%]"
        # å¤„ç†å¯èƒ½çš„ç¼–ç é—®é¢˜å’Œç©ºå€¼ï¼Œä»¥åŠåŒ…å«æ¢è¡Œç¬¦çš„å­—æ®µ
        print("æ­£åœ¨å¤„ç†å±åŒ–å“æ¸…å•æ•°æ®...")
        df_hazardous[product_name_col] = df_hazardous[product_name_col].astype(str).fillna('')
        
        # ç»Ÿè®¡åŒ…å«æ¢è¡Œç¬¦çš„è®°å½•æ•°
        newline_count_before = df_hazardous[product_name_col].str.contains(r'[\n\r]', regex=True).sum()
        if newline_count_before > 0:
            print(f"ğŸ”§ å‘ç° {newline_count_before} æ¡å“åè®°å½•åŒ…å«æ¢è¡Œç¬¦ï¼Œæ­£åœ¨æ¸…ç†...")
        
        # å¤„ç†åŒ…å«æ¢è¡Œç¬¦çš„å“åå­—æ®µï¼šå°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œç„¶åæ¸…ç†å¤šä½™ç©ºæ ¼
        df_hazardous[product_name_col] = df_hazardous[product_name_col].str.replace('\n', ' ', regex=False)
        df_hazardous[product_name_col] = df_hazardous[product_name_col].str.replace('\r', ' ', regex=False)
        df_hazardous[product_name_col] = df_hazardous[product_name_col].str.replace(r'\s+', ' ', regex=True).str.strip()
        
        # åŒæ ·å¤„ç†åˆ«åå­—æ®µ
        if hazard_alias_col != product_name_col:
            df_hazardous[hazard_alias_col] = df_hazardous[hazard_alias_col].astype(str).fillna('')
            alias_newline_count = df_hazardous[hazard_alias_col].str.contains(r'[\n\r]', regex=True).sum()
            if alias_newline_count > 0:
                print(f"ğŸ”§ å‘ç° {alias_newline_count} æ¡åˆ«åè®°å½•åŒ…å«æ¢è¡Œç¬¦ï¼Œæ­£åœ¨æ¸…ç†...")
            df_hazardous[hazard_alias_col] = df_hazardous[hazard_alias_col].str.replace('\n', ' ', regex=False)
            df_hazardous[hazard_alias_col] = df_hazardous[hazard_alias_col].str.replace('\r', ' ', regex=False)
            df_hazardous[hazard_alias_col] = df_hazardous[hazard_alias_col].str.replace(r'\s+', ' ', regex=True).str.strip()
        
        df_hazardous['å“å_çº¯å‡€'] = df_hazardous[product_name_col].str.replace(r'\[.*?\]', '', regex=True).str.strip()
        df_hazardous['æµ“åº¦é˜ˆå€¼'] = df_hazardous[product_name_col].str.extract(r'(\[.*?\])', expand=False).fillna('')
        
        # å»é™¤é‡å¤çš„å“å_çº¯å‡€ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª
        df_hazardous = df_hazardous.drop_duplicates(subset=['å“å_çº¯å‡€'], keep='first')
        print(f"å±åŒ–å“æ¸…å•å…±æœ‰ {len(df_hazardous)} æ¡æœ‰æ•ˆè®°å½•")

        # åˆå¹¶ç©ºè¡Œå¤„ç†
        print("ğŸ”§ æ­£åœ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†...")
        df_input = merge_empty_rows(df_input, chinese_name_col, input_cas_col)

        # åˆ›å»ºæ–°çš„åˆ—ï¼Œå¹¶ç”¨é»˜è®¤å€¼å¡«å……
        print("æ­£åœ¨å¤„ç†å¾…åŒ¹é…æ•°æ®...")
        df_input['åˆ«å'] = ''
        df_input['æ˜¯å¦ä¸ºå±åŒ–å“'] = 'å¦'
        df_input['æµ“åº¦é˜ˆå€¼'] = ''

        # å†æ¬¡åˆå¹¶åˆ«ååˆ—ï¼ˆå¤„ç†æ–°åˆ›å»ºçš„"åˆ«å"åˆ—ï¼‰
        print("ğŸ”§ æ­£åœ¨å¤„ç†æ–°åˆ›å»ºçš„åˆ«ååˆ—...")
        df_input = merge_alias_columns(df_input, "å¾…å¤„ç†æ•°æ®")

        # å¤„ç†è¾“å…¥æ•°æ®ä¸­å¯èƒ½çš„ç¼–ç é—®é¢˜
        # ç¡®ä¿æ£€æµ‹åˆ°çš„ä¸­æ–‡ååˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        df_input[chinese_name_col] = df_input[chinese_name_col].astype(str).fillna('')

        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæå‡æ€§èƒ½ï¼Œåˆ›å»ºå±åŒ–å“å­—å…¸ç”¨äºå¿«é€ŸæŸ¥æ‰¾
        # éœ€è¦åŒæ—¶ä¿å­˜åˆ«åå’Œæµ“åº¦é˜ˆå€¼ä¿¡æ¯
        hazardous_dict = df_hazardous.set_index('å“å_çº¯å‡€')[['æµ“åº¦é˜ˆå€¼', hazard_alias_col]].to_dict('index')
        
        # å¦‚æœæœ‰CASå·åˆ—ï¼Œåˆ›å»ºåŸºäºCASå·çš„å­—å…¸ç”¨äºè¾…åŠ©åŒ¹é…
        cas_dict = {}
        if input_cas_col and hazard_cas_col:
            # æ¸…ç†CASå·æ•°æ®ï¼Œå¤„ç†å¯èƒ½çš„æ¢è¡Œç¬¦
            df_hazardous[hazard_cas_col] = df_hazardous[hazard_cas_col].astype(str).fillna('').str.strip()
            df_hazardous[hazard_cas_col] = df_hazardous[hazard_cas_col].str.replace('\n', '', regex=False)
            df_hazardous[hazard_cas_col] = df_hazardous[hazard_cas_col].str.replace('\r', '', regex=False)
            df_hazardous[hazard_cas_col] = df_hazardous[hazard_cas_col].str.replace(r'\s+', '', regex=True)
            
            # åˆ›å»ºCASå·åˆ°åŒ–å­¦å“ä¿¡æ¯çš„æ˜ å°„
            for _, row in df_hazardous.iterrows():
                cas_no = row[hazard_cas_col]
                if cas_no and cas_no.lower() not in ['nan', '', 'null']:
                    cas_dict[cas_no] = {
                        'å“å_çº¯å‡€': row['å“å_çº¯å‡€'],
                        'æµ“åº¦é˜ˆå€¼': row['æµ“åº¦é˜ˆå€¼'],
                        hazard_alias_col: row[hazard_alias_col]
                    }
            print(f"ğŸ“Š åˆ›å»ºäº† {len(cas_dict)} ä¸ªCASå·æ˜ å°„")
        
        # æ¸…ç†ä¸­æ–‡åæ•°æ®
        df_input['ä¸­æ–‡å_æ¸…ç†'] = df_input[chinese_name_col].astype(str).fillna('').str.strip()
        
        # å¦‚æœæœ‰CASå·åˆ—ï¼Œä¹Ÿæ¸…ç†CASå·æ•°æ®
        if input_cas_col:
            df_input['CASå·_æ¸…ç†'] = df_input[input_cas_col].astype(str).fillna('').str.strip()
            df_input['CASå·_æ¸…ç†'] = df_input['CASå·_æ¸…ç†'].str.replace('\n', '', regex=False)
            df_input['CASå·_æ¸…ç†'] = df_input['CASå·_æ¸…ç†'].str.replace('\r', '', regex=False) 
            df_input['CASå·_æ¸…ç†'] = df_input['CASå·_æ¸…ç†'].str.replace(r'\s+', '', regex=True)
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œè¿›è¡ŒåŒ¹é…
        def match_chemical(row):
            name = row['ä¸­æ–‡å_æ¸…ç†'] if 'ä¸­æ–‡å_æ¸…ç†' in row.index else ''
            cas_no = row['CASå·_æ¸…ç†'] if 'CASå·_æ¸…ç†' in row.index else ''
            
            # ä¼˜å…ˆä½¿ç”¨CASå·åŒ¹é…ï¼ˆæ›´å‡†ç¡®ï¼‰
            if cas_no and cas_no in cas_dict:
                match_info = cas_dict[cas_no]
                alias_name = match_info[hazard_alias_col]
                concentration = match_info['æµ“åº¦é˜ˆå€¼']
                hazard_name = match_info['å“å_çº¯å‡€']  # è·å–å±åŒ–å“æ¸…å•ä¸­çš„å“å
                
                # å¦‚æœåˆ«åä¸ºç©ºæˆ–NaNï¼Œä¿æŒä¸ºç©ºå­—ç¬¦ä¸²
                if pd.isna(alias_name) or str(alias_name).strip() == '' or str(alias_name).lower() == 'nan':
                    alias_name = ''
                else:
                    alias_name = str(alias_name).strip()
                
                # æ£€æŸ¥ä¸­æ–‡åæ˜¯å¦åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…åˆ™è¿”å›ä¿®æ­£åçš„ä¸­æ–‡å
                corrected_name = hazard_name if name != hazard_name else ''
                
                return pd.Series(['æ˜¯', alias_name, concentration, corrected_name])
            
            # å¦‚æœCASå·æ²¡æœ‰åŒ¹é…åˆ°ï¼Œä½¿ç”¨ä¸­æ–‡ååŒ¹é…
            elif name and name in hazardous_dict:
                # è·å–åˆ«åå’Œæµ“åº¦é˜ˆå€¼
                alias_name = hazardous_dict[name][hazard_alias_col]
                concentration = hazardous_dict[name]['æµ“åº¦é˜ˆå€¼']
                
                # å¦‚æœåˆ«åä¸ºç©ºæˆ–NaNï¼Œä¿æŒä¸ºç©ºå­—ç¬¦ä¸²
                if pd.isna(alias_name) or str(alias_name).strip() == '' or str(alias_name).lower() == 'nan':
                    alias_name = ''
                else:
                    alias_name = str(alias_name).strip()
                
                # ä¸­æ–‡ååŒ¹é…æ—¶ä¸éœ€è¦ä¿®æ­£
                return pd.Series(['æ˜¯', alias_name, concentration, ''])
            
            else:
                return pd.Series(['å¦', '', '', ''])
        
        # åº”ç”¨åŒ¹é…å‡½æ•°ï¼Œç›´æ¥æ›´æ–°ä¸­æ–‡åˆ«ååˆ—
        result_cols = ['æ˜¯å¦ä¸ºå±åŒ–å“', 'ä¸­æ–‡åˆ«å', 'æµ“åº¦é˜ˆå€¼', 'ä¸­æ–‡åä¿®æ­£']
        if input_cas_col:
            # å¦‚æœæœ‰CASå·åˆ—ï¼Œä¼ é€’åŒ…å«ä¸­æ–‡åå’ŒCASå·çš„æ•°æ®
            df_input[result_cols] = df_input[['ä¸­æ–‡å_æ¸…ç†', 'CASå·_æ¸…ç†']].apply(match_chemical, axis=1)
        else:
            # å¦‚æœæ²¡æœ‰CASå·åˆ—ï¼Œåªä¼ é€’ä¸­æ–‡å
            df_input[result_cols] = df_input['ä¸­æ–‡å_æ¸…ç†'].apply(lambda name: match_chemical(pd.Series({'ä¸­æ–‡å_æ¸…ç†': name})))
        
        # å¤„ç†ä¸­æ–‡åä¿®æ­£ï¼šå¦‚æœä¸­æ–‡åä¿®æ­£ä¸ä¸ºç©ºï¼Œåˆ™æ›´æ–°åŸå§‹ä¸­æ–‡ååˆ—
        has_corrections = df_input['ä¸­æ–‡åä¿®æ­£'] != ''
        correction_count = has_corrections.sum()
        
        if correction_count > 0:
            print(f"ğŸ”§ å‘ç° {correction_count} æ¡è®°å½•éœ€è¦ä¸­æ–‡åä¿®æ­£ï¼ˆCASå·åŒ¹é…ä½†ä¸­æ–‡åä¸åŒ¹é…ï¼‰")
            
            # åˆ›å»ºåˆå¹¶åçš„ä¸­æ–‡ååˆ—ï¼šä»¥å±åŒ–å“ç›®å½•ä¸­çš„åç§°ä¸ºå‡†
            # å¯¹äºæœ‰ä¿®æ­£çš„è®°å½•ï¼Œä½¿ç”¨å±åŒ–å“ç›®å½•ä¸­çš„æ ‡å‡†åç§°
            # å¯¹äºæ²¡æœ‰ä¿®æ­£çš„è®°å½•ï¼Œä¿æŒåŸå§‹ä¸­æ–‡å
            df_input['åˆå¹¶ä¸­æ–‡å'] = df_input[chinese_name_col].copy()
            df_input.loc[has_corrections, 'åˆå¹¶ä¸­æ–‡å'] = df_input.loc[has_corrections, 'ä¸­æ–‡åä¿®æ­£']
            
            # å°†åˆå¹¶åçš„ä¸­æ–‡åæ›¿æ¢åŸå§‹ä¸­æ–‡ååˆ—
            df_input[chinese_name_col] = df_input['åˆå¹¶ä¸­æ–‡å']
            
            # åˆ é™¤ä¸´æ—¶åˆ—
            df_input.drop('åˆå¹¶ä¸­æ–‡å', axis=1, inplace=True)
            
            print(f"âœ… å·²å°† {correction_count} æ¡è®°å½•çš„ä¸­æ–‡åæ›´æ–°ä¸ºå±åŒ–å“ç›®å½•ä¸­çš„æ ‡å‡†åç§°")
        
        # åˆ é™¤ä¸´æ—¶çš„ä¸­æ–‡åä¿®æ­£åˆ—
        df_input.drop('ä¸­æ–‡åä¿®æ­£', axis=1, inplace=True)
        
        # åˆ é™¤ä¸´æ—¶åˆ—
        df_input.drop('ä¸­æ–‡å_æ¸…ç†', axis=1, inplace=True)
        if input_cas_col:
            df_input.drop('CASå·_æ¸…ç†', axis=1, inplace=True)
        
        # ç»Ÿè®¡ç»“æœ
        total_rows = len(df_input)
        matched_count = len(df_input[df_input['æ˜¯å¦ä¸ºå±åŒ–å“'] == 'æ˜¯'])
        corrected_count = correction_count if 'correction_count' in locals() else 0
        print(f"å¤„ç†å®Œæˆï¼šå…±å¤„ç† {total_rows} æ¡è®°å½•ï¼ŒåŒ¹é…åˆ°å±åŒ–å“ {matched_count} æ¡")
        if corrected_count > 0:
            print(f"å…¶ä¸­ {corrected_count} æ¡è®°å½•çš„ä¸­æ–‡åå·²æ ¹æ®CASå·è¿›è¡Œäº†ä¿®æ­£")

        # è°ƒæ•´åˆ—çš„é¡ºåºï¼Œç¡®ä¿ä¸­æ–‡åˆ«ååœ¨åˆé€‚çš„ä½ç½®
        print("æ­£åœ¨è°ƒæ•´åˆ—é¡ºåº...")
        cols = df_input.columns.tolist()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´ä¸­æ–‡åˆ«ååˆ—çš„ä½ç½®
        if 'ä¸­æ–‡åˆ«å' in cols:
            # æ‰¾åˆ°ä¸­æ–‡ååˆ—çš„ä½ç½®
            cn_name_index = cols.index(chinese_name_col)
            chinese_alias_index = cols.index('ä¸­æ–‡åˆ«å')
            
            # ç¡®å®šç†æƒ³çš„æ’å…¥ä½ç½®ï¼ˆä¸­æ–‡åä¹‹åï¼‰
            ideal_position = cn_name_index + 1
            
            # å¦‚æœå­˜åœ¨è‹±æ–‡ååˆ—ï¼Œåœ¨è‹±æ–‡åå‰æ’å…¥ä¸­æ–‡åˆ«å
            if english_name_col and english_name_col in cols:
                en_name_index = cols.index(english_name_col)
                # å¦‚æœè‹±æ–‡ååœ¨ä¸­æ–‡åä¹‹åï¼Œåˆ™åœ¨è‹±æ–‡åå‰æ’å…¥
                if en_name_index > cn_name_index:
                    ideal_position = en_name_index
                print(f"ğŸ“‹ ä¸­æ–‡åˆ«ååˆ—ä½ç½®è°ƒæ•´ï¼šåœ¨ {chinese_name_col} å’Œ {english_name_col} ä¹‹é—´")
            else:
                print(f"ğŸ“‹ ä¸­æ–‡åˆ«ååˆ—ä½ç½®è°ƒæ•´ï¼šåœ¨ {chinese_name_col} ä¹‹å")
            
            # å¦‚æœä¸­æ–‡åˆ«ååˆ—ä¸åœ¨ç†æƒ³ä½ç½®ï¼Œåˆ™è°ƒæ•´
            if chinese_alias_index != ideal_position and chinese_alias_index != ideal_position - 1:
                # ç§»åŠ¨ä¸­æ–‡åˆ«ååˆ—åˆ°ç†æƒ³ä½ç½®
                cols.insert(ideal_position, cols.pop(chinese_alias_index))
                df_input = df_input[cols]
                print(f"âœ… å·²è°ƒæ•´ä¸­æ–‡åˆ«ååˆ—ä½ç½®")
            else:
                print(f"âœ… ä¸­æ–‡åˆ«ååˆ—ä½ç½®æ— éœ€è°ƒæ•´")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡åˆ«ååˆ—ï¼Œè·³è¿‡åˆ—é¡ºåºè°ƒæ•´")

        # ä¿å­˜å¤„ç†åçš„æ–‡ä»¶ï¼Œå¦‚æœè¡Œæ•°è¶…è¿‡5000ï¼Œåˆ™åˆ†å—ä¿å­˜
        print("æ­£åœ¨ä¿å­˜å¤„ç†ç»“æœ...")
        output_dir = Path(os.path.dirname(output_file))
        output_dir.mkdir(parents=True, exist_ok=True)

        chunk_size = 200

        if total_rows > chunk_size:
            num_chunks = (total_rows + chunk_size - 1) // chunk_size
            print(f"ğŸ“Š æ•°æ®é‡è¾ƒå¤§ ({total_rows} è¡Œ)ï¼Œå°†åˆ† {num_chunks} ä¸ªæ–‡ä»¶ä¿å­˜ (æ¯ä»½ {chunk_size} è¡Œ)")

            base_filename = Path(output_file).stem
            output_extension = Path(output_file).suffix

            for i in range(num_chunks):
                start_row = i * chunk_size
                end_row = start_row + chunk_size
                chunk_df = df_input.iloc[start_row:end_row]

                # ç”Ÿæˆåˆ†å—æ–‡ä»¶å
                chunk_filename = f"{base_filename}_part_{i+1}{output_extension}"
                chunk_output_path = output_dir / chunk_filename

                # ä¿å­˜åˆ†å—æ–‡ä»¶
                chunk_df.to_csv(chunk_output_path, index=False, encoding=OUTPUT_ENCODING)
                print(f"   - âœ… å·²ä¿å­˜åˆ†å—æ–‡ä»¶: {chunk_output_path.name}")

            print(f"âœ… å¤„ç†å®Œæˆï¼Œæ‰€æœ‰åˆ†å—æ–‡ä»¶å·²ä¿å­˜è‡³: {output_dir}")
        else:
            # å¦‚æœæ•°æ®é‡ä¸å¤§ï¼Œç›´æ¥ä¿å­˜
            df_input.to_csv(output_file, index=False, encoding=OUTPUT_ENCODING)
            print(f"âœ… å¤„ç†å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜è‡³: {output_file}")

        print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶ç¼–ç : {OUTPUT_ENCODING}")
        match_rate = (matched_count / total_rows * 100) if total_rows > 0 else 0
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: æ€»è®°å½•æ•° {total_rows}ï¼Œå±åŒ–å“ {matched_count} æ¡ï¼ŒåŒ¹é…ç‡ {match_rate:.1f}%")
        print(f"â±ï¸  æ€»ç”¨æ—¶: {time.time() - start_time:.2f} ç§’")
        
        return True, matched_count, total_rows

    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        return False, 0, 0
    except ValueError as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False, 0, 0
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False, 0, 0

def process_all_files():
    """æ‰¹é‡å¤„ç†æ‰€æœ‰å¾…å¤„ç†æ–‡ä»¶"""
    print("=" * 80)
    print("ğŸ§ª åŒ–å­¦å“æ•°æ®æ‰¹é‡å¤„ç†å·¥å…· v2.1")
    print("=" * 80)
    
    # å®šä¹‰æ–‡ä»¶è·¯å¾„ - æ•°æ®æ–‡ä»¶åœ¨çˆ¶ç›®å½•ä¸­
    current_dir = Path(__file__).parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
    input_dir = current_dir / 'å¾…å¤„ç†æ•°æ®'
    hazardous_list_path = current_dir / 'å›½å®¶å±åŒ–å“æ¸…å•' / 'å›½å®¶å±åŒ–å“æ¸…å•.csv'
    output_dir = current_dir / 'å·²å®ŒæˆåŸºç¡€å¤„ç†çš„æ–‡ä»¶'

    # æ£€æŸ¥å±åŒ–å“æ¸…å•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not hazardous_list_path.exists():
        print(f"âŒ å±åŒ–å“æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {hazardous_list_path}")
        return False

    # æŸ¥æ‰¾æ‰€æœ‰å¾…å¤„ç†çš„æ•°æ®æ–‡ä»¶ï¼ˆCSVå’ŒExcelï¼‰
    data_files = find_data_files(input_dir)
    
    if not data_files:
        print(f"âŒ åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ•°æ®æ–‡ä»¶")
        print(f"ğŸ“‹ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {', '.join(SUPPORTED_EXTENSIONS)}")
        return False
    
    print(f"ğŸ“ æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶å¾…å¤„ç†:")
    for i, file_path in enumerate(data_files, 1):
        print(f"   {i}. {file_path.name}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_files = len(data_files)
    successful_files = 0
    total_records = 0
    total_hazardous = 0
    failed_files = []
    
    # æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶
    for i, input_file_path in enumerate(data_files, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“ æ­£åœ¨å¤„ç†ç¬¬ {i}/{total_files} ä¸ªæ–‡ä»¶: {input_file_path.name}")
        print(f"{'='*60}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_filename = generate_output_filename(input_file_path.name)
        output_file_path = output_dir / output_filename
        
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        success, hazardous_count, record_count = process_data(
            str(input_file_path), 
            str(hazardous_list_path), 
            str(output_file_path)
        )
        
        if success:
            successful_files += 1
            total_records += record_count
            total_hazardous += hazardous_count
            print(f"âœ… æ–‡ä»¶ {input_file_path.name} å¤„ç†æˆåŠŸ")
        else:
            failed_files.append(input_file_path.name)
            print(f"âŒ æ–‡ä»¶ {input_file_path.name} å¤„ç†å¤±è´¥")
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡ç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ“ˆ æ‰¹é‡å¤„ç†å®Œæˆ - æ€»ç»“æŠ¥å‘Š")
    print(f"{'='*80}")
    print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"âœ… æˆåŠŸå¤„ç†: {successful_files}")
    print(f"âŒ å¤„ç†å¤±è´¥: {len(failed_files)}")
    print(f"ğŸ“Š æ€»è®°å½•æ•°: {total_records:,}")
    print(f"ğŸ§ª æ€»å±åŒ–å“: {total_hazardous}")
    if total_records > 0:
        print(f"ğŸ“ˆ æ€»ä½“åŒ¹é…ç‡: {total_hazardous/total_records*100:.2f}%")
    
    if failed_files:
        print(f"\nâŒ å¤„ç†å¤±è´¥çš„æ–‡ä»¶:")
        for file_name in failed_files:
            print(f"   - {file_name}")
    
    if successful_files == total_files:
        print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†æˆåŠŸå®Œæˆï¼")
        return True
    elif successful_files > 0:
        print(f"\nâš ï¸  éƒ¨åˆ†æ–‡ä»¶å¤„ç†å®Œæˆï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„æ–‡ä»¶ã€‚")
        return True
    else:
        print(f"\nğŸ’¥ æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return False

def detect_column_names(df, column_type):
    """
    æ™ºèƒ½æ£€æµ‹DataFrameä¸­çš„åˆ—å
    
    Args:
        df (pd.DataFrame): æ•°æ®æ¡†
        column_type (str): åˆ—ç±»å‹ ('chinese_name', 'english_name', 'product_name')
    
    Returns:
        str or None: æ£€æµ‹åˆ°çš„åˆ—åï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
    """
    # å®šä¹‰å¯èƒ½çš„åˆ—åæ¨¡å¼
    patterns = {
        'chinese_name': [
            'ä¸­æ–‡å', 'ä¸­æ–‡åç§°', 'åŒ–å­¦å“ä¸­æ–‡å', 'ç‰©è´¨ä¸­æ–‡å', 'äº§å“ä¸­æ–‡å',
            'ä¸­æ–‡', 'åç§°', 'åŒ–å­¦åç§°', 'ç‰©è´¨åç§°', 'äº§å“åç§°', 'chinese_name',
            'cn_name', 'chinese', 'name_cn'
        ],
        'english_name': [
            'è‹±æ–‡å', 'è‹±æ–‡åç§°', 'åŒ–å­¦å“è‹±æ–‡å', 'ç‰©è´¨è‹±æ–‡å', 'äº§å“è‹±æ–‡å',
            'è‹±æ–‡', 'english_name', 'en_name', 'english', 'name_en',
            'chemical_name', 'substance_name'
        ],
        'product_name': [
            'å“å', 'äº§å“å', 'ç‰©è´¨å', 'åŒ–å­¦å“å', 'åç§°', 'product_name',
            'chemical_name', 'substance_name', 'name', 'å“åç§°'
        ],
        'alias': [
            'åˆ«å', 'åˆ«ç§°', 'ä¿—å', 'é€šç”¨å', 'å•†å“å', 'alias', 'aliases',
            'alternative_name', 'common_name', 'trade_name', 'synonym'
        ],
        'cas': [
            'CASå·', 'CAS', 'caså·', 'cas', 'CAS_NO', 'cas_no', 'CAS-NO',
            'cas_number', 'CAS_Number', 'registry_number', 'CAS_RN'
        ]
    }
    
    # è·å–å¯¹åº”ç±»å‹çš„å¯èƒ½åˆ—å
    possible_names = patterns.get(column_type, [])
    
    # æ£€æŸ¥åˆ—åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    df_columns_lower = [col.lower().strip() for col in df.columns]
    
    for pattern in possible_names:
        pattern_lower = pattern.lower().strip()
        # ç²¾ç¡®åŒ¹é…
        if pattern_lower in df_columns_lower:
            return df.columns[df_columns_lower.index(pattern_lower)]
        
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
        for i, col in enumerate(df_columns_lower):
            if pattern_lower in col or col in pattern_lower:
                return df.columns[i]
    
    return None

def display_dataframe_info(df, name, detected_cols=None):
    """
    æ˜¾ç¤ºæ•°æ®æ¡†çš„åŸºæœ¬ä¿¡æ¯
    
    Args:
        df (pd.DataFrame): æ•°æ®æ¡†
        name (str): æ•°æ®æ¡†åç§°
        detected_cols (dict): æ£€æµ‹åˆ°çš„åˆ—ä¿¡æ¯
    """
    print(f"\nğŸ“Š {name} æ•°æ®ä¿¡æ¯:")
    print(f"   ğŸ“ æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"   ğŸ“‹ æ‰€æœ‰åˆ—å: {list(df.columns)}")
    
    if detected_cols:
        print(f"   ğŸ¯ æ£€æµ‹åˆ°çš„å…³é”®åˆ—:")
        for col_type, col_name in detected_cols.items():
            if col_name:
                print(f"      - {col_type}: {col_name}")
            else:
                print(f"      - {col_type}: æœªæ£€æµ‹åˆ°")
    
    # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®çš„é¢„è§ˆ
    print(f"   ğŸ‘€ æ•°æ®é¢„è§ˆ:")
    print("   " + str(df.head(2).to_string()).replace('\n', '\n   '))
    print()

def main():
    """ä¸»å‡½æ•°"""
    return process_all_files()

if __name__ == '__main__':
    main()
